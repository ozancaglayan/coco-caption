#!/usr/bin/env python
import sys
import json
import argparse

from pathlib import Path

import tabulate

from pycocotools.coco import COCO
from pycocoevalcap.eval import COCOEvalCap

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        prog='cocoeval',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description="COCO Evaluation Script")

    parser.add_argument('-g', '--gts', type=str, required=True,
                        help="Ground-truth COCO style .json file")
    parser.add_argument('-r', '--res', nargs='+',
                        help="COCO style results annotation .json file(s)")
    parser.add_argument('-m', '--metric', default=None,
                        help="Dumps the requested metric for single system evaluation.")
    parser.add_argument('-w', '--write-scores', action='store_true',
                        help="Dump the score per each system into a .json file.")

    args = parser.parse_args()

    if args.metric and len(args.res) > 1:
        print('Error: -m should be used with only one system')
        sys.exit(1)

    # Create COCO object with the ground-truth
    coco = COCO(args.gts)

    headers = ['Bleu_1', 'Bleu_4', 'METEOR', 'CIDEr', 'ROUGE_L']
    scores = []

    for result in args.res:
        path = Path(result)
        model_name = path.name
        score_file = Path(str(path.name) + '.score')
        if len(args.res) > 1:
            # Only print when multiple systems are given
            print('Evaluating {} ...'.format(model_name))
        # Check whether the results were cached
        if args.write_scores and score_file.exists():
            score_dict = json.load(open(score_file))
        else:
            coco_res = coco.loadRes(result)
            coco_eval = COCOEvalCap(coco, coco_res)
            coco_eval.params['image_id'] = coco_res.getImgIds()
            score_dict = coco_eval.evaluate(verbose=False)
            if args.write_scores:
                with open(score_file, 'w') as f:
                    json.dump(score_dict, f)

        if args.metric:
            # Single system should have been given
            print(score_dict[args.metric])
            sys.exit(0)

        scores.append(
            [model_name.replace('.json', '')] + [score_dict[m] for m in headers],
        )

    # Sort by METEOR
    scores = sorted(scores, key=lambda x: x[3])
    print(tabulate.tabulate(scores, headers=headers, floatfmt='.3f'))
